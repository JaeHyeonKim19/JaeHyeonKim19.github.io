---
title: '[컴퓨터구조론] 5. Large and Fast: Exploiting Memory Hierachy'
date: 2020-10-04 09:00:00 +0900
path: 'computerArchitecture/202010045-large-and-fast-exploiting-memory-hierarchy'
---

> 본 글은 영남대학교 최규상 교수님의 [컴퓨터 구조](http://www.kocw.net/home/cview.do?cid=184062fa9a833237) 강의를 듣고 작성된 글입니다.

### 5.1 Introduction

- Principle of Locality
	- Programs access a small proportion of their address space at any time
	- Temporal locality
		- Items accessed recently are likely to be accessed again soon
		- e.g., instructions in a loop, induction variables
	- Spatial locality
		- Items near those accessed recently are likely to be accessed soon
		- e.g., sequential instruction access, array data

- Taking Advantage of Locality
	- Memory hierarchy
	- Store everything on disk
	- Copy recently accessed (and nearby) items from disk to smaller DRAM memory
		- Main memory
	- Copy more recently accessed (and nearby) items from DRAM to smaller SRAM memory
		- Cache memory attached to CPU

- Memory Hierarchy Levels
	![memory-hierachy-levels](./memory-hierarchy-levels.png)
	- Block (aka line): unit of copying
		- May be multiple words
	- If accessed data is present in upper level
		- Hit: access satisfied by upper level
			- Hit ratio: hits/accesses
	- If accessed data is absent
		- Miss: block copied from lower level
			- Time taken: miss penalty
			- Miss ratio: misses/accesses = 1 - hit ratio
		- Then accessed data supplied from upper level

- Characteristics of the Memory Hierarchy
	![characteristics-of-the-memroy-hierarchy](./characteristics-of-the-memory-hierarchy.png)

- DRAM Technology
	- Data stored as a charge in a capacitor
		- Single transistor used to access the charge
		- Must periodically be refreshed
			- Read contents and write back
			- Performed on a DRAM "row"
	![dram-technology](./dram-technology.png)

- Advanced DRAM Organization
	- Bits in a DRAM are organized as a rectangular array
		- DRAM accesses an entire row
		- Burst mode: supply successive words from a row with reduced latency
	- Double data rate (DDR) DRAM
		- Transfer on rising and falling clock edges
	- Quad data rate (QDR) DRAM
		- Separate DDR inputs and outputs

- DRAM Generations
	- 시간이 지남에따라 가격은 낮아지고 용량은 증가하였음

- DRAM Performance Factors
	- Row buffer
		- Allows several words to be read and refreshed in parallel
	- Synchronous DRAM
		- Allows for consecutive accesses in bursts without needing to send each address
		- Improves bandwidth
	- DRAM banking
		- Allows simultaneous access to multiple DRAMs
		- Improves bandwidth

- Main Memory Supporting Caches
	- Use DRAMs for main memory
		- Fixed width (e.g., 1 word)
		- Connected by fixed-width clocked bus
			- Bus clock is typically slower than CPU clock
	- Example cache block read
		- 1 bus cycle for address transfer
		- 15 bus cycles per DRAM access
		- 1 bus cycle per data transfer
	- For 4-word block, 1-word-wide DRAM
		- Miss penalty = 1 + 4 * 15 + 4 * 1 = 65 bus cycles
		- Bandwidth = 16 bytes / 65 cycles = 0.25 B/cycle

- Increasing Memory Bandwidth
	![increasing-memory-bandwidth](./increasing-memory-bandwidth.png)

### 5.2 Memory Technologies

- Review: Major Components of a Comupter
	![review-major-components-of-a-computer](review-major-components-of-a-computer.png)

- Memory Technology
	- Static RAM (SRAM)
		- 0.5ns ~ 2.5ns, $2000 ~ $5000 per GB
	- Dynamic RAM (DRAM)
		- 50ns ~ 70ns, $20 ~ $75 per GB
	- Magnetic disk (HD)
		- 5ms ~ 20ms, $0.20 ~ $2 per GB
	- Ideal memory
		- Access time of SRAM
		- Capacity and cost/GB of disk

- Processor-Memory Performance Gap
	- 프로세서와 메모리의 성능 차이는 시간이 지날수록 양극화되어 왔다.

- The "Memory Wall"
	- Processor vs DRAM speed disparity continues to grow
	![the-memory-wall](./the-memory-wall.png)
	- Good memory hierachy (cache) design is increasingly important to overall performance

### 5.3 The Basics of Caches

- Cache Memory
	- Cache memory
		- The level of the memory hierarchy closest to the CPU
	- Given accesses X1, ..., Xn-1, Xn
	- How do we know if the data is present?
	- Where do we look?
	![cache-memory](./cache-memory.png)
	
- Caching: A Simple First Example
	![caching-a-simple-first-example](./caching-a-simple-first-example.png)

- Direct Mapped Cache
	- Location determined by address
	- Direct mapped: only one choice
		- (Block address) modulo (#Blocks in cache)
	- #Block is a power of 2
	- Use low-order address bits
	![direct-mapped-cache](./direct-mapped-cache.png)

- Tags and Valid Bits
	- How do we know which particular block is stored in a cache location?
		- Store block address as well as the data
		- Actually, only need the high-order bits
		- Called the tag
	- What if there is no data in a location?
		- Valid bit: 1 = present, 0 = not present
		- Initially 0

- MIPS Direct Mapped Cache Example
	- One word blocks, cache size = 1K words (or 4KB)
	![mips-direct-mapped-cache-example](./mips-direct-mapped-cache-example.png)

- Example: Larger Block Size
	- 64 blocks, 16 bytes/block
		- To what block number does address 1200 map?
	- Block address = 1200 / 16 = 75
	- Block number = 75 modulo 64 = 11
	![example-larger-block-size](./example-larger-block-size.png)

- Multiword Block Direct Mapped Cache
	- Four word/block, cache size = 1K words
	![multiword-block-direct-mapped-cache](./multiword-block-direct-mapped-cache.png)

- Block Size Considerations
	- Larger blocks should reduce miss rate
		- Due to spatial locality
	- But in a fixed-sized cache
		- Larger blocks -> fewer of them
			- More competition -> increased miss rate
		- Larger blocks -> pollution
	- Larger miss penalty
		- Can override benefit of reduced miss rate
		- Early restart and critical-word-first can help

- Cache Misses
	- On cache hit, CPU proceeds normally
	- On cache miss
		- Stall the CPU pipeline
		- Fetch block from next level of hierachy
		- Instruction cache miss
			- Restart instruction fetch
		- Data cache miss
			- Compleete data access

- Write-Through
	- On data-write hit, could just update the block in cache
		- But then cache and memory would be inconsistent
	- Write through: also update memory
	- But makes writes take longer
		- e.g., if base CPI = 1, 10% of instructions are stores, write to memory takes 100 cycles
			- Effective CPI = 1 + 0.1 * 100 = 11
	- Solution: write buffer
		- Holds data waiting to be written to be written memory
		- CPU continues immediately
			- Only stalls on write if write buffer is already full

- Write-Back
	- Alternative: On data-write hit, just update the block in cache
		- Keep track of whether each block is dirty
	- When a dirty block is replaced
		- Write it back to memory
		- Can use a write buffer to allow replacing block to be read first

- Wirte Allocation
	- What should happen on a write miss?
	- Alternatives for write-through
		- Allocate on miss: fetch the block
		- Write around: don't fetch the block
			- Since programs often write a whole block before reading it (e.g., initialization)
	- For write-back
		- Usually fetch the block

- Example: Intrinsity FastMATH
	- Embedded MIPS processor
		- 12-stage pipeline
		- Instruction and data access on each cycle
	- Split cache: separate I-cache and D-cache
		- Each 16KB: 256 blocks * 16 words/block
		- D-cache: write-through or write-back
	- SPEC2000 miss rates
		- I-cache: 0.4%
		- D-cache: 11.4%
		- Weighted average: 3.2%
	![example-instrinsity-fastmath](./example-intrinsity-fast-math.png)